\documentclass[10pt]{article}
\usepackage[usenames]{color} %used for font color
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage[utf8]{inputenc} %useful to type directly diacritic characters
\begin{document}
\[%Chapter 4
\renewcommand{\thechapter}{4}
\chapter{Models and Uncertainty}
\label{chap:models and uncertainty}

\begin{linenumbers}
\section{Models}
\label{sec:models}
The word 'model' is used frequently within this text to describe different entities.  Because of this, great care must be taken to define what a model is and which model is being discussed.  At its heart, a model is a representation of a real world entity, phenomenon, or system.  Models can be scaled representations of a real object, such as model cars or airplane, or a mathematical model, such as a weather model.  Models are used to represent reality and are used to study or represent a thought, hypothesis, or theory.  The purpose in model building is to create a reasonable, relevant representation of reality.

\citet{Oates98largedatasets} stated that data size and model size are linearly related while model accuracy does not necessarily significantly increase.  One economist argues that complex macro-scaled models based on micro-scaled concepts have yet to replace some basic, foundational concepts in use for over 50 years.  Smaller, simpler, macro-scaled models based on macro-scaled principles are just as effective while having a shorter development and implementation time.  This also holds true in ecological studies \citep{Adkinson2009}.   Models that include more data and complexity than can be justified by the quality or quantity of the combined available data may result in decreased predictive power, or at best may be no better than a simpler model \citep{Ginzburg2003, bridgeland2008business}. With these concepts in mind, whenever possible, models were simplified to the greatest extent possible to avoid unnecessary model bloating and over characterization of reality.

\clearpage{}
\section{Computational Models}
\label{sec:computational models}
Three types of model are discussed in this thesis: computational models, regression models, and probability models.  Computational models are used to calculate the water balance and selenium loading.  These models are formed based on known physical concepts and are not estimates.  Regression models use regression methods to estimate computational model input variables.  Probability models are used to characterize uncertainty.  There are eight computational models discussed in this thesis.  They are as follows:
\begin{itemize}
	\item	Upstream Study Reach Water Balance Deterministic Model
	\item	Upstream Study Reach Water Balance Stochastic Model
	\item	Upstream Study Reach Selenium Mass Balance Deterministic Model
	\item	Upstream Study Reach Selenium Mass Balance Stochastic Model
	\item	Downstream Study Reach Water Balance Deterministic Model
	\item	Downstream Study Reach Water Balance Stochastic Model
	\item	Downstream Study Reach Selenium Mass Balance Deterministic Model
	\item	Downstream Study Reach Selenium Mass Balance Stochastic Model
\end{itemize}

These models can be grouped by three different categories: location (Upstream Study Reach or Downstream Study Reach), material (water or selenium), and model type (deterministic or stochastic).  The first two categories are self explanatory.  The third, model type, requires further discussion.

Deterministic models are used throughout every calculable aspect of science, including all model sizes and degrees of complexity.  What defines a deterministic model is what it doesn't have; uncertainty.  Whether that uncertainty is from measurement, spatial and/or temporal variability, or calculation error.  Deterministic models are solved analytically and, if given the same starting data, will always produce the same results.

Stochastic models are used to model spatial-temporal effects or the effects due to measurement error.  If the same starting data is used, stochastic models will return different, unique, results with each run.  Each one of these unique results is a realization of the model.  Stochastic models require multiple runs to determine the probability distribution of the results.  The distribution of results provides the scope and magnitude of the uncertainty contained within the complete model.  Stochastic models are usually solved in an iterative manner using methods such as Monte Carlo simulation, scenario optimization, and various random search methods.

%The eight computational models presented in the preceding list have one characteristic in common; all eight are time series models.  The reference time frames are identical for all eight models.  The time frame spans from 1 October 2006 to 30 September 2010.  The start and end dates are concurrent with the start of the 2007 water year and the end of the 2010 water year as defined by the USGS, respectively \citep{USGS2014}.
%
%Since most of the data was readily available in average daily form, it seemed reasonable to set the time step within the time frame equal to one day.  The added benefit is that the results could be used to help calibrate MODFLOW and RT3D models developed under the same project for the same study regions as described in this thesis.
%
%Not all time steps with the time frame were found to be suitable for analysis.  A time step was deemed suitable only if all of the data points required for the respective selenium mass balance deterministic model were available.  It was determined that attempting to calculate with any number of missing data points or attempting to estimate the missing data points for a given time step would add additional immeasurable uncertainty to the results.  The number of required variables is 23 for the USR and 14 for the DSR.  For the USR, 1,085 of 1,461 days, or approximately 74\%, were found to be suitable for inclusion in the analysis.  For the DSR, 838 days, or approximately 57\% were found to be suitable.

\clearpage{}
\section{Linear and Non-Linear Regression Models}
\label{sec:linear and non-linear regression models}
Computational models are one of the three model types presented in this thesis.  The second model type, regression models, are used to determine a best fit equation to some set of independent and dependent variables.  There are two different regression models categories:
\begin{itemize}
	\item Linear regression models
	\item Non-linear regression models
\end{itemize}
%
%Linear regression models are defined as models where the functions of the predictor variables are not themselves variable.  This is shown in equation \ref{eq:LinearModel}.  A regression model is considered linear if $f_i$ does not contain any fitting parameters $(\beta_i)$.
%\begin{equation}
%\label{eq:LinearModel}
%\hat{y}=\beta_0+\beta_1 f_1+\beta_2 f_2+\ldots+\beta_n f_n
%\end{equation}
%\begin{tabular}{r l}
%Where:&\\
%$\hat{y}$ = & fitted or predicted value\\
%$\beta_i$ = & fitting parameters\\
%$f_i$ = & functions of the predictor variables $x_i$\\
%\end{tabular}\\
%
%Whenever possible, ordinary least squares regression was used to generate best fit equations with a given set of independent variables.  Pearson's r-squared value is used as an initial goodness-of-fit value so that individual linear regression models can be evaluated both independently and comparatively with regards to how well they fit the data.  R-squared values for linear models are positive, non-negative values less than one (1) with one (1) indicating a perfect fit.  R-squared values account for the percent of the dependent variable that can be accounted for in the independent variables.  The adjusted r-squared value is calculated and compared to the r-squared value.  This allows for some accounting for uncertainty when using multiple independent variables. If the two are considerably different, then the estimating model is missing an explanatory independent variable \citep{Johnson2007}.

%The f-statistic was also calculated and compared to the critical f-statistic for each estimating model. An f-statistic greater than the critical f-statistic indicates that at least one of the explanatory independent variables is linearly associated with the calculated dependent variable.  When comparing estimating models for suitability, the model with the greater f-statistic is more suitable \citep{Johnson2007}.  The model f-statistic is returned as part of the statistics software linear regression summary.  The critical f-statistic is calculated using the f-distribution and the desired significance level and the degrees of freedom.

%The significance level, $\alpha$, is closely tied to the desired confidence interval for this study.  Considering the number and source of the input variables, it was considered desirable to have all models calculated to account for 95\% of the variability.  This means that 5\% of the variability in any model can be attributed to chance.  With all models being two-tailed, the central inter-percentile range (CIR) was calculated for the range between 2.5\% and 97.5\% as a means to comprehend the daily change in variability.  This takes the 5\% unaccounted variability and distributes it to the two tails.  The $\alpha$ is not changed on account of the two-tailed nature of the models.

%The two-sided p-value was used to determine if the estimating model independent variables were statistically significant.  The p-values greater than $\alpha$ indicated that the independent variable was not significant and did not contribute significantly to describe the variability of the dependent variable.  These variables were considered for removal during linear regression model optimization.

%Non-linear regression models are those models where the function of the predictor variable, $f_i$, contains a fitting parameter, $\beta_i$.  Non-linear regression models were used only when a specific model form could be determined from known physical or geometrical relationships.  R-squared values were not used to determine goodness-of-fit for non-linear regression models.  These models can have valid values that are negative or greater than one \citep{spiess2010evaluation} and as such are outside of the boundary for comparing linear models.  Pseudo or modified r-squared calculations are available.  These computations result in values that are comparable to the r-squared value for linear models, but have slightly different interpretations (reference).  Since non-linear regression models were used only when specific model forms could be pre-determined, there was no need to compare different model forms estimating the same result.
%
%In order to define non-linear regression model goodness-of-fit, the root mean squared error (RMSE) value was calculated.  The RMSE "represents the standard deviation of the differences between he predicted and observed values" \citep{Wiki:RMSE}.  The RMSE is scale dependent as the units are the same as the observed value.  The RMSE is also known as the standard deviation.  This would cause an issue if models for different observed value units and scales were compared against each other.  In this study, non-linear regression models are only used to estimate the cross-sectional width of a river segment.  The development and discussion of this calculation will be discussed in later sections.  This thesis does not contain any other non-linear regression models.  This allows us to compare the residual errors associated with the various cross sections without needing to consider scale or units.

\clearpage{}
\section{Distribution Models}
\label{sec:distribution models}
Computational models and regression models are two of the three model types presented in this thesis.  The third model type, distribution models, are used to provide a range of possible values for a given variable.  There are two different distribution models categories:
\begin{itemize}
	\item Uni-variate distribution models
	\item Non-parametric distribution models
\end{itemize}
%
%Uni-variate distribution models are crucial to describing the uncertainty of individual parameters within this study.  Uni-variate distribution models describe the uncertainty of a single variable, independent of any other variable.
%
%Defining uncertainty is quite cumbersome as each uncertainty term comes with its own distinct definition.  The definitions usually come in the form of a distribution type with associated factors with each distribution type having its own particular mathematical definition with its own factors.  Distributions require as few as one factor.  When a distribution had more than one definition, only standard, accepted definition was used.  This thesis does not discuss the computations of the different distributions.  The primary concern regarding distributions is to determine which of the well established distributions best fit the data.
%
%Uncertainty and error appear to have synonymous meanings.  In this study they have distinctly different definitions.  This study defines error as the difference between two individual values.  When used in reference to the input variables, error refers to the difference between a specific measured value and its associated true value.  When used in reference to regression, error refers to the difference between the predicted value and measured or observed value.  Uncertainty is defined as the distribution of possible error values for a given parameter.
%
%The true value of the parameter is not directly observable and is only valid for a specific time and location.  The measured value is the value that is measured and recorded.  The difference between the two values is the error.  Error sources are both systematic and random.  The relationship between the measured value, true value, and error is described in equation \ref{eq:error}.
%\begin{equation}
%\label{eq:error}
%mv=tv+\varepsilon
%\end{equation}
%\begin{tabular}{r l}
%Where:&\\
%mv = & measured value\\
%tv = & true value\\
%$\varepsilon$ = & error\\
%\end{tabular}\\
%
%If the true value were known, then it would be a simple matter of algebra to determine the error from any measured value.  This practice is used in manufacturing when a part or item is designed to a specific dimension.  The measured values incorporate a certain amount of error due to manufacturing variances and measurement error.  In this case, the sought after value is the error value to determine the efficiency of the manufacturing process.
%
%In the case of this study, the true value is never truly known, it can only be approximated based on a measured value.  Based on the measured values and the known instrument variability, it is possible to estimate the distribution of the uncertainty.  The U.S. Geological Survey and the Colorado Department of Water Resources have developed uncertainty distributions for many of the measured values they report.  The particulars of these distributions will be discussed later.
%
%Figure \ref{fig:TV-MVError} is a graphical representation of a pair of possible scenarios when considering true and measured values.  In all of the graphs, the uncertainty distribution is shown as the dashed black curve.  The vertical black line at zero indicates the error associated with the true value.  The vertical red and blue lines indicates the error for the two scenarios.  The red line error value is near the black line.  This indicates that the error is small and the measured value is near the true value.  The blue line error value is a significant distance from the true value, but is within the uncertainty distribution.  This indicates that the error is large and the measured value is significantly different than the true value.  Both scenarios are plausible as the error values are within the uncertainty distribution associated with the true value.  Graphs 1 and 2 show the red and blue scenarios.
%\begin{figure}[htbp]
%\begin{center}
%	\includegraphics[width=6in]{"Figures/TV-MV Error"}
%	\caption[Comparison of measured value and true value]{Comparison of measured value and true value.  The black vertical line depicts the error associated with the true value.  The black dashed line is the distribution of the uncertainty of the measured value from the true value.  The red and blue vertical lines depict two measured value scenarios.  The red and blue dashed curved depict the uncertainty distributions applied to the associated measured values.  The grey area is representative of the probability that the true value and measured value are from the same distribution.}
%	\label{fig:TV-MVError}
%\end{center}
%\end{figure}
%
%If we apply algebra to equation \ref{eq:error} and solve for the true value, we get equation \ref{eq:error2}.
%\begin{equation}
%\label{eq:error2}
%tv=mv+\varepsilon
%\end{equation}
%
%The results of this equation are shown on graphs 3 and 4 of figure \ref{fig:TV-MVError}.  In both of the scenarios, the uncertainty distribution of the true value is applied to the measured values and is depicted as dashed red and blue curves.  In the red scenario where the error is small, depicted in graph 3, the overlap of the true value uncertainty distribution and the measured value uncertainty distribution is large and are almost identical.  In the blue scenario, depicted in graph 4, the overlap is small.  The overlap areas in graphs 3 and 4 are shaded gray.  These gray areas are representative of the probability that the true value and measured value are from the same distribution.
%
%Ideally, the true value would be estimated based on the measured value and the relationship between the true value and the particular measured value.  Since the true value is not known, the only reasonable course is to estimate the true value as in equation \ref{eq:error2}.  Additional uncertainty can be attributed to this approach, but the uncertainty would be variable based on the difference between the true and measured value.  Since this difference is unknown, it is unreasonable to guess.
%
%Non-parametric distribution models are used as an aid for analyzing uni-variate data sets.  Specifically, kernel density estimates are used in conjunction with histograms to assist in visual analysis of the data.  Figure \ref{fig:ExampleDistAnalysis} is an example of a random sample of one of the input data sets used in this thesis.  The curve is the kernel density estimate.  The short vertical lines between the histogram and the x-axis, called a rug, depict the data values.  This figure adequately displays the resulting differences between histograms and kernel density estimates.  Kernel density estimates can more accurately depict data groupings that are lost in histogram bins.  The histogram leads us to believe that the data has a strong tendency to be near zero, wile the kernel density shows that the majority of the data is between 0-20.  Histograms can more accurately depict extremes or cut-off values.  In the figure, there are no values less than zero.  The histogram clearly shows this while the kernel density estimate shows that there are values less than zero.  Both histograms and kernel density estimates are used throughout this thesis to assist in the description of distributions.  A rug is also presented with the histogram whenever the quantity of data allows for adequate data presentation.  A rug is not included when the data set is too large to allow for discreet identification of data values.
%\begin{figure}[htbp]
%\begin{center}
%	\includegraphics[width=6in]{"Figures/Example KDE"}
%	\caption[Example kernel density estimate.]{Example kernel density estimate.  The data is a random sample of an input variable used in this thesis.  The curve depicts the kernel density estimate.  The short vertical lines between the histogram and the x-axis, called a rug, depict the data values.}
%	\label{fig:ExampleDistAnalysis}
%\end{center}
%\end{figure}
%
%\clearpage{}
%\section{Uncertainty of In Stream Data}
%\label{sec:uncertainty of in stream data}
%The U.S. Geological Survey (USGS) and the Colorado Department of Water Resources (CDWR) provides data for stream flow, water temperature, and specific conductance measurements required for the mass balance analysis.  Each data set they provide has its own uncertainty distribution.  The USGS has a standardized methodology to characterize the uncertainty with specific data sets.  This methodology was adopted by the Colorado Department of Water Resources.
%
%Stream flow data is provided in either 15-minute increments or average daily flow format, where average daily flow is the average of all of the 15-minute increment measurements taken during a calendar day.  The USGS provides three specific uncertainty distributions for average daily stream flow data which are described in every annual water data report \citep{USGS2006NWIS, USGS2007NWIS, USGS2008NWIS, USGS2009NWIS, USGS2010NWIS, USGS2011NWIS, USGS2012NWIS}.  The USGS calls these uncertainty distributions "ratings of accuracy".  These ratings are "excellent", "good", and "fair".  The "excellent" rating indicates that "95 percent of the daily discharges are within 5 percent of the true value".  The distributions expand to 10\% and 15\% for "good" and "fair" ratings, respectively.  There is a fourth rating used on some average daily stream flow data gathered in the LARB; "poor".  This rating is not specific as it states that 95 percent of the values are more than 15\% of the true value.  For this thesis, we have assumed that the "poor" rating indicates that 95 percent of the values are within 20 percent of the true value.
%
%Average daily stream flow is calculated using equation \ref{eq:flow and error1}.   
%\begin{equation}
%\label{eq:flow and error1}
%	\bar{Q}=\bar{Q}_{reported}+\varepsilon_Q
%\end{equation}
%\begin{tabular}{r l}
%Where:&\\
%$\bar{Q}$ =& estimated probable flow rate,\\
%$\bar{Q}_{reported}$ =& reported flow rate,\\
%$\varepsilon_Q$ =& flow rate error.\\
%\end{tabular}\\
%
%Using this equation with very low flow values may result in negative flow rates.  Therefore, $\bar{Q}$ was calculated using a truncated normal distribution where the minimum allowed value was \SI{0}{\cubic\meter\per\second}, the maximum allowed value was 1.25 times the maximum reported value for the specific stream gauge, and the mean value was the reported flow rate value.  Truncation was performed using an algorithm within the statistical software that used re-sampling and replacement.  Other methods were available, but they were deemed to be less reliable.
%
%Water temperature and specific conductance data is also collected concurrently with the 15-minute increment stream flow measurements.  These two data sets also have ratings assigned to them using the terms "excellent", "good", "fair", and "poor".  The quantity of values within these distributions is not specifically described as for the stream flow data.  Since the uncertainty ratings use the same terms, we assume that these ratings encompass 95 percent of the values, just like the stream flow data.  The range of the ratings are dependent on the parameter being measured and are based on a combination of instrument fouling and calibration drift for the specific instrument.  Table \ref{tab:USGSRatings} is extracted from a USGS Annual Water Data Report and describes the ranges assigned to each of the measured parameters and accuracy ratings.
%\begin{table}[htbp]
%\begin{center}
%	\caption[USGS Ratings of Accuracy Table.]{USGS Ratings of Accuracy Table.  Extracted from a USGS Annual Water Data Report.}
%	\label{tab:USGSRatings}
%	\includegraphics[width=6in]{"Figures/USGS_error_table"}
%\end{center}
%\end{table}
%
%While not specifically stated by the USGS or the CDWR, the ratings described are assumed to be normally distributed.  The accuracy rating descriptions provided by the USGS do not allow for straight forward creation of a normal distribution using the mean and standard deviation.  The mean of the distribution is assumed to be the measured value.  To determine the standard deviation, we begin by assuming that the "95\% of the values" is centrally located.  This means that the distribution encompasses the range between the 2.5th and 97.5th percentile on a normal distribution.  Since the measured value is assumed to be the mean, the distribution encompasses a range between $0.025\cdot \mu$ and $1.975 \cdot \mu$.  A quick examination of a Z-score table shows that this equates to a range of Z-scores between -1.96 and 1.96.  Starting with the Z-score equation, \ref{eq:Zscore}:
%\begin{equation}
%	\label{eq:Zscore}
%	Z=\frac{x-\mu}{\sigma}
%\end{equation}
%\begin{tabular}{r l}
%Where:&\\
%$Z$ = & Z-score\\
%$x$ = & measured value\\
%$\mu$ = & distribution mean\\
%$\sigma$ = & distribution standard deviation\\
%\end{tabular}\\
%
%Substituting the $Z$, $x$, and $\mu$ values where $x$ is the deviation of $\mu$ at the 97.5th percentile and solving for $\sigma$:
%\begin{align}
%	\nonumber 3=&\frac{x-\mu}{\sigma}\\
%	\sigma=&\frac{x-\mu}{1.96}
%\end{align}
%
%The same solution is found when using values for the 2.5th percentile.  Defining $x$ for a distribution range expressed as an absolute value, as in the case of temperature, results in equation \ref{eq:Zscore4} where $\mu+v$ is the maximum possible value and $v$ is the maximum difference from the mean value.  
%\begin{equation}
%	\label{eq:Zscore4}
%	\sigma=\frac{(\mu+v)-\mu}{1.96}=\frac{v}{1.96}
%\end{equation}
%
%When defined for a distribution rage expressed as a percentage of the reported value, equation \ref{eq:Zscore5} is the result where $\mu(1+p)$ is the maximum possible value and $p$ is the maximum decimal percent difference from the mean value.
%\begin{equation}
%	\label{eq:Zscore5}
%	\sigma=\frac{\mu(1+p)-mu}{1.96}=\frac{\mu \cdot p}{1.96}
%\end{equation}
%
%This procedure provides a standard method for setting the parameters of the uncertainty distributions associated with a specified measurements provided by the USGS and the CDWR.
%
%Reported stream flow depth measurements reflect the average daily stream depth at the gauge sites.  The USGS and CDWR adhere to the same standard whereby stream flow depth measurements are within  \SI{\pm 0.003}{\meter} (\SI{\pm 0.01}{\foot}).  It is assumed that this error range describes the 95\% central inter-percentile range of a normal distribution.
%
%\clearpage{}
%\section{Uncertainty of Atmospheric Data}
%\label{sec:uncertainty of atmospheric data}
%Unlike the in stream data, atmospheric data obtained from CoAgMet does not have publicly defined uncertainty distributions.  At the time this thesis, a study headed by Dr.\ Jos\'{e} L.\ Ch\'{a}vez at Colorado State University was underway to evaluate the American Society of Civil Engineers Environmental and Water Resources Institute (ASCE EWRI) standardized alfalfa ET reference Penman-Monteith equation (ASCE-ET equation) with relation to lysimetric data in Colorado.  Preliminary findings provided by Dr. Ch\'{a}vez indicate an average daily reference ET mean bias error of $-0.47~mm \cdot day^{-1}$ with a variability of $\pm 0.98~mm \cdot day^{-1}$ when compared to lysimetric data.  The bias error indicates that the reported $ET_{ref}$ underestimates the actual ET.  The measurement variability is reported as the Root Mean Square Error (RMSE), or standard deviation, of the measurement.
%
%Precipitation in the study regions is not well defined.  An assumption was made that precipitation was not uniformly distributed over either study region.  Reports from field technicians, conversation with local residents, and personal experience indicate that most storms during the summer were isolated and relatively small.  It was not known by the end of this study whether weather radar data was available or could be used to more accurately estimate rainfall onto the Arkansas R. water surface.  It was assumed that, at best, 50\% of any rain event would add water directly to the river.
%
%Precipitation, minimum daily relative humidity, and wind speed data error provided through the CoAgMet system was based solely on instrument accuracy.  Instrument accuracy was reported as $\pm1\%$ for precipitation, $\pm2\%$ for minimum daily relative humidity, and $\pm0.5~m \cdot s^{-1}$ for wind speed.  These values are based on the typical instruments installed at the various locations.  Sensors vary somewhat between observation sites due to equipemnt replacement.  Colorado Climate Network personnel have stated that replacement equipment either meets or exceeds the equipment on a typical observation site.  The uncertainty distributions for these three weather measurements was assumed to be normal with 95\% of the values within the reported range of the true value (coagmet website).  Additional sources of uncertainty regarding these values are included in the preliminary results reported by Dr.\ Ch\'{a}vez.
%
%$ET_{ref}$ and precipitation were calculated with respect to the surface of the Arkansas R.  Storm runoff, evaporation from bank evaporation, and evapotranspiration from plant life within the river channel were not included in the calculation.  Storm runoff is difficult to estimate on a regional basis for a non-uniform rainfall event.  It was assumed that for the vast majority of rainfall events, surface runoff into the river would be negligible.  Evaporative losses from bank evaporation were neglected because this evaporation is outside of the study boundary.  While there are areas where vegetation is within the river's boundaries, transpiration losses are neglected because their magnitude and effect on the study region are not known.  All three of these neglected quantities are included in the unaccounted for flow definition in section

%\clearpage{}
%\section{Uncertainty of Lab Data}
%\label{sec:uncertainty of lab data}
%Dissolved selenium concentration lab analysis results are subject to a variety of error sources.
%
%As described in section \ref{sec:field data collection} dissolved selenium samples were periodically taken as duplicate samples.  For each 'A' and 'B' sample pair, each sample was compared to the mean of the 'A' and 'B' sample.  Figure \ref{fig:CSe uncertainty} shows the comparisons analyzed.  The top graph plots the 'A' and 'B' sample concentrations against the difference from the mean of the 'A' and 'B' samples.  The bottom graph plots the same data, but with respect to the percent difference from the mean of the 'A' and 'B' samples.
%\begin{figure}
%\begin{center}
%	\includegraphics[width=6in]{"Figures/Results_USR/CSe Error Scatter"}
%	\caption[Dissolved Selenium Concentration Variation]{Dissolved Selenium Concentration Variation.}
%\end{center}
%\end{figure}
%
%\begin{figure}[htbp]
%\begin{center}
%	\includegraphics[width=6in]{"Figures/Results_USR/CSe Error"}
%	\caption[Dissolved Selenium Concentration Uncertainty.]{Dissolved Selenium Concentration Uncertainty.  Each sub-figure shows the comparison of the lab reported dissolved selenium concentration and the mean for each pair of duplicate samples.}
%	\label{fig:CSe uncertainty}
%\end{center}
%\end{figure}
%
%The percent difference between the reported lab values and the mean duplicates was best fit with a logistic distribution.  The logistic distribution had a location parameter of -0.067 and a scale parameter of 1.807.  This comparison and distribution combination was chosen because it had the best fit when compared to others using Kolmogorov-Smirnov, Cramer-von Mises, and Anderson-Darling goodness-of-fit test statistics.  The duplicate samples had a mean difference from the mean of their respective 'A' and 'B' samples of 0\% with a standard deviation of 4.16\%.  This corresponds to 95\% of the data within $\pm$10\% of the reported value
%
%Calculated dissolved selenium concentrations at specific sites were constrained to fit between one-half of the minimum historically reported value and 1-1/2 of the maximum historically reported value.  This range should allow for dissolved selenium concentration variations that are comparable to the values reported from the field samples.  The range allowed for variation beyond the reported concentration range to allow for the possibility that concentrations beyond the range may possibly have existed at some time.
%
%\clearpage{}
%\section{Computing Tools and Techniques}
%\label{sec:MethCTools}
The R language and computing environment was chosen as the software to be used due to its support and recommendation from the CSU Statistics Department.  It is usable without having a strong computer programming background but has a steep learning curve.  Learning and support are easily obtained through open source publications and internet discussion forums.  The R language and computing environment is free of charge and is open source.  The statistics community at large has strongly supported and participated in its development.  It is a significant statistics software package as it has been implemented by corporations like Google and Merck \citation{Vance2009} for market forecasting and product failure analysis.  All statistical analyses, regression models, and computational models in this study were created and run in R.  The software package R will be written as [R] in the rest of this paper to avoid confusion with the abbreviation 'R.' for river.

Most computations were performed using the 'base' and 'stats' packages that is part of the core programming of [R] and includes all of the statistical analyses discussed with a few exceptions .  Goodness of fit testing was performed using the Kolmogorov-Smirnov (K-S) test statistic, the Cramer-von Mises (CvM) test statistic, and the Anderson-Darling (A-D) test statistic.  These tests are part of the 'fitdistrplus' package.  The Chi-squared statistic is not included because it is only significant for discrete distribution.  Since there are no discrete distributions in this study, the Chi-squared statistic is not used \citep{Laure2012,fitdistrplus} The 'moments' package was used to calculate the skewness and kurtosis statistics \citep{Komsta2012}.  The 'KernSmooth' package was used to create kernel density estimates \citep{kernsmooth}.  Random numbers were drawn from a uni-variate truncated normal distribution using the 'msm' package and a bivariate truncated normal distribution using the 'tmvtnorm' package \citep{msm,kernsmooth}.  Various data handling tasks were performed using the 'data.table' and 'reshape2' packages \citep{datatable,reshape2}.  The 'nlstools' package was used to extract residuals from non-linear least squares regression models \citep{nlstools}.  Various time series management and plotting tasks were performed using the 'zoo' package \citep{zoo}.  Since [R] and the packages used in this study are well supported by the statistics academic and professional communities and are frequently reviewed and updated, it seems unnecessary to discuss the computational methodology and efficacy of either in this paper.

Microsoft Excel spreadsheet software was used to develop the initial models to test the input variables and regression equations for suitability.  This was done to prevent an overly complicated estimating equation from limiting the number of days with suitable results.  Equations and formulas in the models were not allowed to be unless all input variables were present.  It was reasoned that the absence of a single numeric input variable would increase the uncertainty of the final model to an unknown degree.  

The models developed and tested on spreadsheets were then converted into computational models in [R] where stochastic modeling of input variables and distributions of residual error could be applied.  Initially, the USR selenium loading model was created.  This model is the most complex of any of the other models with the highest number of input variables.  

The stochastic models use Monte Carlo (MC) simulation techniques.  The first model was created such that solutions for individual realizations were calculated individually.  This model's computational form was extremely slow in producing fairly acceptable results.  It took approximately 2.5 days to calculate a little over 7,500 realizations with only five of the six monitored statistics reaching the desired point.  For some reason, the final change in skewness between successive groups of realizations was 3-5\% based on five model runs.  A second model was created such that the solutions were calculated in a batch format using matrix math.  Many sources with extensive and diverse computer code development knowledge stated that matrix math operations complete computations many orders of magnitude faster than the one calculation at a time method used in the first model.  This proved to be true as a 5,000 realization model was completed in just under 2.25 hours.

One run was of the second stochastic model form was performed at 500 realizations to determine the minimum number of required realizations.  The mean, 2.5th and 97.5th percentile, variance, skewness, and kurtosis were the monitored statistics.  The statistics were considered acceptable when the change between calculations of the 1 to $r$ realization and the 1 to $r-1$ realization was less than 0.1\%.  It was found that these variables were acceptable or nearly acceptable shortly before the 500th realization.  A large factor of safety was added to this and 5000 realizations were performed.

Deterministic models used the reported values from the input variables.  In all cases, the reported values were considered the expected values.  Therefore, the terms deterministic model value, reported value, and expected value were used interchangeably in this study as they all refer to the same values.  Stochastic model input variables were defined from distributions of the reported values.  They were generated as $t$ rows by $r$ columns matrices where $t$ was the number of time steps in the study time frame and $r$ was the number of realizations.

The stochastic models were derived from the deterministic models with the addition of error terms.  The stochastic model was then converted into a computational stochastic model.  This models was debugged and validated by testing the random error distributions and comparing the final results to previous studies.  The computational deterministic models were then derived from the computational stochastic models by removing all of the error distributions from the computational code.  The deterministic models were then compared to the original spreadsheet models.

Hardware was the limiting factor for the second model.  Large models required more internal memory to complete the calculations.  Large whole chunks of memory were assigned for each matrix used in each calculation.  This required a more high-end personal computer to run the models.  While not an issue to find, it was necessary to schedule computer resource use against user requirements.

Undoubtedly, there are regression equations and calculation methods that can more accurately and quickly describe any single calculated value, but the added complexity reduces the quantity of useful results.  As one of the purposes of this study is to produce results for comparing and calibrating other computational models, it is necessary to temper the desire to have seemingly high accuracy results at the expense of having a low volume of resulting data.
\clearpage{}
\end{linenumbers}\]
\end{document}